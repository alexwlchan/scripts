#!/usr/bin/env python3
"""
This is a "greedy" script to compress a directory into a tar.gz.

I use it when I'm in a space-constrained environment (e.g. AWS CloudShell)
where I have enough space to hold the uncompressed files OR the compressed
archive, but not both.  It compresses the directory and deletes the
original files as it goes.

Use with caution; I only use it when I can easily recreate the original.

"""

import os
import sys
import tarfile

import tqdm


def get_file_paths_under(root=".", *, suffix=""):
    """Generates the paths to every file under ``root``."""
    if not os.path.isdir(root):
        raise ValueError(f"Cannot find files under non-existent directory: {root!r}")

    for dirpath, _, filenames in os.walk(root):
        for f in filenames:
            if os.path.isfile(os.path.join(dirpath, f)) and f.lower().endswith(suffix):
                yield os.path.join(dirpath, f)


if __name__ == '__main__':
    try:
        dirname = sys.argv[1]
    except IndexError:
        sys.exit(f"Usage: {__file__} <DIRNAME>")

    if not os.path.isdir(dirname):
        sys.exit(f"Usage: {__file__} <DIRNAME>")

    with tarfile.open(os.path.basename(dirname.strip('/')) + '.tar.gz', 'w:gz') as tar:
        for path in tqdm.tqdm(list(get_file_paths_under(dirname))):
            tar.add(path, arcname=os.path.relpath(path, dirname), recursive=True)
            os.unlink(path)